{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "spark: SparkSession = (\n",
    "    SparkSession.builder.appName(\"data_exploration\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setCheckpointDir(\"../checkpoints/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set schema and timeStampFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = (\n",
    "    \"polarity FLOAT, id LONG, date_time TIMESTAMP, query STRING, user STRING, text STRING\"\n",
    ")\n",
    "timestampformat = \"EEE MMM dd HH:mm:ss zzz yyyy\"\n",
    "\n",
    "spark_reader = spark.read.schema(schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark_reader.csv(\n",
    "    \"../data/raw_data/training.1600000.processed.noemoticon.csv\",\n",
    "    quote=\"'\",\n",
    "    header=False,\n",
    "    encoding=\"utf-8\",\n",
    "    timestampFormat=timestampformat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+--------+---------------+--------------------+\n",
      "|polarity|        id|          date_time|   query|           user|                text|\n",
      "+--------+----------+-------------------+--------+---------------+--------------------+\n",
      "|     0.0|1467810369|2009-04-07 08:49:45|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|     0.0|1467810672|2009-04-07 08:49:49|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|     0.0|1467810917|2009-04-07 08:49:53|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|     0.0|1467811184|2009-04-07 08:49:57|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|     0.0|1467811193|2009-04-07 08:49:57|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|     0.0|1467811372|2009-04-07 08:50:00|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|     0.0|1467811592|2009-04-07 08:50:03|NO_QUERY|        mybirch|         Need a hug |\n",
      "|     0.0|1467811594|2009-04-07 08:50:03|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|     0.0|1467811795|2009-04-07 08:50:05|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|     0.0|1467812025|2009-04-07 08:50:09|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "+--------+----------+-------------------+--------+---------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- polarity: float (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.limit(10).show()\n",
    "dataframe.printSchema()\n",
    "dataframe.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample a small subset of dataframe to simulate streaming procedure <br>and subtract it from main dataframe in order to create static dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_dataframe = dataframe.sampleBy(\n",
    "    \"polarity\", fractions={0: 0.1875, 4: 0.1875}\n",
    ").cache()\n",
    "\n",
    "static_dataframe = dataframe.subtract(streaming_dataframe).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+--------+--------------+--------------------+\n",
      "|polarity|        id|          date_time|   query|          user|                text|\n",
      "+--------+----------+-------------------+--------+--------------+--------------------+\n",
      "|     0.0|1467811594|2009-04-07 08:50:03|NO_QUERY|          coZZ|@LOLTrish hey  lo...|\n",
      "|     0.0|1467812025|2009-04-07 08:50:09|NO_QUERY|       mimismo|@twittera que me ...|\n",
      "|     0.0|1467812964|2009-04-07 08:50:22|NO_QUERY|lovesongwriter|Hollis' death sce...|\n",
      "|     0.0|1467813579|2009-04-07 08:50:31|NO_QUERY|    starkissed|@LettyA ahh ive a...|\n",
      "|     0.0|1467815923|2009-04-07 08:51:07|NO_QUERY|     fatkat309|some1 hacked my a...|\n",
      "|     0.0|1467817502|2009-04-07 08:51:32|NO_QUERY|       Tmttq86|@fleurylis I don'...|\n",
      "|     0.0|1467818603|2009-04-07 08:51:49|NO_QUERY|     kennypham|Sad, sad, sad. I ...|\n",
      "|     0.0|1467819022|2009-04-07 08:51:56|NO_QUERY|   hpfangirl94|Falling asleep. J...|\n",
      "|     0.0|1467819712|2009-04-07 08:52:06|NO_QUERY|     labrt2004|Just checked my u...|\n",
      "|     0.0|1467821338|2009-04-07 08:52:30|NO_QUERY|   justnetgirl|Put vacation phot...|\n",
      "+--------+----------+-------------------+--------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_dataframe.limit(10).orderBy('date_time', ascending=True).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Details of sampled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299520"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|polarity| count|\n",
      "+--------+------+\n",
      "|     0.0|149582|\n",
      "|     4.0|149938|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_dataframe.groupby(\"polarity\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save sampled dataframe ordered by date and time; also, <br>because I want to read this file without Spark, I saved it in one partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_dataframe.orderBy('date_time', ascending=True).coalesce(1).write.csv(\n",
    "    \"../data/streaming_data\", mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Details of static dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300480"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|polarity| count|\n",
      "+--------+------+\n",
      "|     4.0|650062|\n",
      "|     0.0|650418|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_dataframe.groupby(\"polarity\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze text column to see potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|@Toddly00 lucky, I was up at 7:30am                                                                                                       |\n",
      "|@omgcorrine im sorry honey  are you guys fighting?                                                                                        |\n",
      "|@mitchelmusso i wanna go but i can't                                                                                                      |\n",
      "|hate to say this cause i love @marthastewart 1 bowl chocolate but those coconut cupcakes kinda suck (p29). a waste of time and resources. |\n",
      "|@uncommon_sense I'm still waiting on my #canlit flask from @smack416 - I'd trade my harddrive to've had one during #NXNE                  |\n",
      "|@sazychik I just saw Honduras play 2 weeks ago, so I would choose Real Madrid but man i cant afford the trip                              |\n",
      "|@ssamantha Odd since Google is going out of their way to censor 'pr0n' and such.                                                          |\n",
      "|Doing school for a couple hours  American Idol sure took alot outta me last night! I love you Danny Gokey &lt;3                           |\n",
      "|@shiloh_dawg will you really? Thank you  Youtube videos aren't helping haha.                                                              |\n",
      "|@PetaEdwards  thanks very much!                                                                                                           |\n",
      "|@nan_jones fine and how bout' u?                                                                                                          |\n",
      "|Testing out adding some gaming news to my @fTag group using _XZUM tag ~ Mega Man 2.5D Trailer ~ http://digg.com/d1sOo5                    |\n",
      "|@BisForBecca Senior Tie  Cant believe it, im guessin ur not going to school as ur not on twitter :-p Talk to u tonight  or on the bus.    |\n",
      "|@voicesinthepine welcome to twitter  Let me know if you need any help.                                                                    |\n",
      "|@shadybrady You wanna rip me some of that Vampire Weekend goodness?  Happy Mooonday!!                                                     |\n",
      "|@cubedweller Gah - not for me on freeview then  Still - if they brought over the Colbert Report I'd consider switching just to get that.  |\n",
      "|@XeniaCarone oh ok that's too bad  are you an actor?                                                                                      |\n",
      "|@christinaesmith Sorry to hear that                                                                                                       |\n",
      "|@crimsong19 I hate ticketmaster. They always screw things up.                                                                             |\n",
      "|@WonderboyLB no babes. Gotta unpack or it will never happen. Busy week here with lakers in playoffs.                                      |\n",
      "|@DannysGhirl  not worth me even trying then...pc's taking 4ever to load rr up                                                             |\n",
      "|looking 4 work &amp; its not looking too good                                                                                             |\n",
      "|@tehpooki3 unfortunately not   add and a migraine aren't a good mix                                                                       |\n",
      "|And the Deore crankset and bottom bracket I was gonna bid on got way too pricey too! It's just not my ebay day  http://bit.ly/13zGAP      |\n",
      "|@imashkaarto18 iya shk me too  shk ternyata kita udh harus standby jam 7! Goshh what time do I hv to wake up! I live in cinere btw d'oohh |\n",
      "|- Now my ex-daycare clients claim that I owe them over $400 and are going to take things to the &quot;next step&quot;.                    |\n",
      "|@RealZoltan @700doo guys i feel like an idiot, u see how selena came to toronto.. i live in toronto  i missed her! dam man!               |\n",
      "|@ddlovato Is like the coolest person in this world and i dont even know her                                                               |\n",
      "|@MaryRSnyder I am seriously going to try.                                                                                                 |\n",
      "|@munirusman Maxcom working good at my home and office...  Highly Recommended...                                                           |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_problems = (\n",
    "    static_dataframe.filter(\n",
    "        f.col(\"text\").contains(\"#\")\n",
    "        | f.col(\"text\").contains(\"http:/\")\n",
    "        | f.col(\"text\").contains(\"@\")\n",
    "        | f.col(\"text\").contains(\"&\")\n",
    "    )\n",
    "    .limit(30)\n",
    "    .select(\"text\")\n",
    ")\n",
    "\n",
    "text_problems.show(30, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if static dataframe has any null values; we can see there is so no any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300480, 1300480)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_dataframe.count(), static_dataframe.na.drop().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prune and clean text column, and change polarity label of 4 to 1.<br> Also, empty texts have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "user_regex = r\"(@\\w{1,15})\"\n",
    "url_regex = r\"((https?|ftp|file):\\/{2,3})+([-\\w+&@#/%=~|$?!:,.]*)|(www.)+([-\\w+&@#/%=~|$?!:,.]*)\"\n",
    "email_regex = r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\"\n",
    "\n",
    "\n",
    "@udf\n",
    "def html_unescape(s: str):\n",
    "    if isinstance(s, str):\n",
    "        return html.unescape(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_static_dataframe = (\n",
    "    static_dataframe.repartition(12)\n",
    "    .withColumn(\"text\", f.regexp_replace(\"text\", url_regex, \"\"))\n",
    "    .withColumn(\"text\", f.regexp_replace(\"text\", email_regex, \"\"))\n",
    "    .withColumn(\"text\", f.regexp_replace(\"text\", user_regex, \"\"))\n",
    "    .withColumn(\"text\", f.regexp_replace(\"text\", \"#\", \" \"))\n",
    "    .withColumn(\"text\", html_unescape(f.col(\"text\")))\n",
    "    .withColumn(\"text\", f.regexp_replace(\"text\", \"[^a-zA-Z']\", \" \"))\n",
    "    .withColumn(\"text\", f.regexp_replace(\"text\", \" +\", \" \"))\n",
    "    .withColumn(\"text\", f.trim(\"text\"))\n",
    "    .filter(\"text != ''\")\n",
    "    .withColumn(\"polarity\", f.when(f.col(\"polarity\") == 4.0, 1.0).otherwise(0.0))\n",
    ").cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Details of clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300461"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_static_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|polarity| count|\n",
      "+--------+------+\n",
      "|     0.0|650413|\n",
      "|     1.0|650048|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_static_dataframe.groupby(\"polarity\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if any errors are left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_problems = (\n",
    "    clean_static_dataframe.filter(\n",
    "        f.col(\"text\").contains(\"#\")\n",
    "        | f.col(\"text\").contains(\"http:/\")\n",
    "        | f.col(\"text\").contains(\"@\")\n",
    "        | f.col(\"text\").contains(\"&\")\n",
    "    )\n",
    "    .select(\"text\")\n",
    ")\n",
    "\n",
    "text_problems.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save static dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_static_dataframe.write.partitionBy('polarity').csv(\n",
    "    \"../data/clean_static_data\", mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
